# -*- coding: utf-8 -*-
"""dataAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RmyCf4boIeH3jivp9I-5xroWC61ryN-S
"""

# Install necessary library
!pip install networkx
import pandas as pd
from dateutil import parser
from dateutil.parser import isoparse
from datetime import datetime, time, timedelta
from networkx.algorithms.dag import descendants

def process_maven_data(csv_file_path):
    # Read 2018 Maven data and convert to a dataframe
    valid_rows = []
    with open(csv_file_path, "r") as file:
        for line in file:
            try:
                row = line.strip().split(",")
                valid_rows.append(row)
            except Exception as e:
                print(f"Skipping row due to error: {e}")

    df = pd.DataFrame(valid_rows)

    # Remove numeric headers
    df.columns = df.iloc[0]
    df = df[1:]
    df = df.rename_axis(None, axis=1)

    # Extract relevant columns
    new_df = df[['Artifact', 'Dependencies', 'Upstream Release Date', 'Dependency Release Date']].copy()

    # Split source and target columns
    new_df[['Source_Group_Id', 'Source_Artifact_Id', 'Source_Version']] = df['Artifact'].str.split(':', expand=True)
    new_df[['Target_Group_Id', 'Target_Artifact_Id', 'Target_Version']] = df['Dependencies'].str.split(':', expand=True)

    df.drop(columns=['Artifact', 'Dependencies'], inplace=True)

    # Rename columns
    df = df.rename(columns={'Upstream Release Date': 'Source Release Date', 'Dependency Release Date': 'Target Release Date'})

    # Add new columns
    df['Source Release Year'] = df['Source Release Date'].str[:4]
    df['Target Release Year'] = df['Target Release Date'].str[:4]

    df = pd.concat([df, new_df], axis=1)

    desired_columns_order = ['Source_Group_Id', 'Source_Artifact_Id', 'Source_Version', 'Source Release Date', 'Source Release Year',
                             'Target_Group_Id', 'Target_Artifact_Id', 'Target_Version', 'Target Release Date', 'Target Release Year']

    df = df[desired_columns_order]

    # Process for G
    df_G = df.drop(columns=['Source_Artifact_Id', 'Target_Artifact_Id'])
    filtered_dfs_G = []
    for year, group in df_G.groupby('Target Release Year'):
        filtered_group_G = group.drop_duplicates(subset=['Target_Group_Id'])
        filtered_dfs_G.append(filtered_group_G)
    filtered_df_G = pd.concat(filtered_dfs_G)

    # Process for GA
    df_GA = df.drop(columns=['Source_Version', 'Target_Version'])
    filtered_dfs_GA = []
    for year, group in df_GA.groupby('Target Release Year'):
        filtered_group_GA = group.drop_duplicates(subset=['Target_Group_Id', 'Target_Artifact_Id'])
        filtered_dfs_GA.append(filtered_group_GA)
    filtered_df_GA = pd.concat(filtered_dfs_GA)

    # Group by target artifact and aggregate data for G
    grouped_G_df = filtered_df_G.groupby(['Target_Group_Id'])
    agg_G_df = grouped_G_df.agg(
        Target_First_Release_Date=('Target Release Date', 'min'),
        Target_Last_Release_Date=('Target Release Date', 'max'),
        Version_Count=('Target_Version', 'count')
    ).reset_index()
    merged_G_df = pd.merge(filtered_df_G, agg_G_df, on=['Target_Group_Id'])

    # Group by target artifact and aggregate data for GA
    grouped_GA_df = filtered_df_GA.groupby(['Target_Group_Id', 'Target_Artifact_Id'])
    agg_GA_df = grouped_GA_df.agg(
        Target_First_Release_Date=('Target Release Date', 'min'),
        Target_Last_Release_Date=('Target Release Date', 'max'),
        Version_Count=('Target_Version', 'count')
    ).reset_index()
    merged_GA_df = pd.merge(filtered_df_GA, agg_GA_df, on=['Target_Group_Id', 'Target_Artifact_Id'])

    # Save to TSV files
    merged_G_df.to_csv('G.tsv', sep='\t', index=False)
    merged_GA_df.to_csv('GA.tsv', sep='\t', index=False)

    return merged_G_df, merged_GA_df

# Run the function with the path to the CSV file
csv_file_path = "/content/drive/MyDrive/maven/2018dataset.csv"
merged_G_df, merged_GA_df = process_maven_data(csv_file_path)